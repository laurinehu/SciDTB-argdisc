Recently , researchers have begun exploring methods 
of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and prompt adherence . <S>
The work on modeling prompt adherence , however , has been focused mainly on whether individual sentences adhere to the prompt . <S>
We present a new annotated corpus of essay-level prompt adherence scores 
and propose a feature-rich approach to scoring essays along the prompt adherence dimension . <S>
Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system 
yielding improvements of up to 16.6 % . <S>
