We provide a comparative study between neural word representations and traditional vector spaces 
based on co-occurrence counts , 
in a number of compositional tasks . <S>
We use three different semantic spaces 
and implement seven tensor-based compositional models , 
which we then test ( together with simpler additive and multiplicative approaches ) in tasks 
involving verb disambiguation and sentence similarity . <S>
To check their scalability , 
we additionally evaluate the spaces 
using simple compositional methods on larger-scale tasks with less constrained language : 
paraphrase detection and dialogue act tagging . <S>
In the more constrained tasks , co-occurrence vectors are competitive , 
although choice of compositional method is important ; 
on the larger-scale tasks , they are outperformed by neural word embeddings , 
which show robust , stable performance across the tasks . <S>
