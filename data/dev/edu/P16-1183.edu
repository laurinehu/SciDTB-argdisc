For many applications , the query speed of N-gram language models is a computational bottleneck . <S>
Although massively parallel hardware like GPUs offer a potential solution to this bottleneck , 
exploiting this hardware requires a careful rethinking of basic algorithms and data structures . <S>
We present the first language model 
designed for such hardware , 
using B-trees 
to maximize data parallelism 
and minimize memory footprint and latency . <S>
Compared with a single-threaded instance of KenLM ( Heafield , 2011 ) , a highly optimized CPUbased language model , 
our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task . <S>
When we saturate both devices , 
the GPU delivers nearly twice the throughput per hardware dollar 
even when the CPU implementation uses faster data structures . <S>
Our implementation is freely available at https : //github.com/XapaJIaMnu/gLM <S>  
