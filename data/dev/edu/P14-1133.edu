A central challenge in semantic parsing is handling the myriad ways 
in which knowledge base predicates can be expressed . <S>
Traditionally , semantic parsers are trained primarily from text 
paired with knowledge base information . <S>
Our goal is to exploit the much larger amounts of raw text 
not tied to any knowledge base . <S>
In this paper , we turn semantic parsing on its head . <S>
Given an input utterance , 
we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each . <S>
Then , we use a paraphrase model to choose the realization 
that best paraphrases the input , 
and output the corresponding logical form . <S>
We present two simple paraphrase models , an association model and a vector space model , 
and train them jointly from question-answer pairs . <S>
Our system PARASEMPRE improves state-of-the-art accuracies on two recently released question-answering datasets . <S>
