While end-to-end neural machine translation ( NMT ) has made remarkable progress recently , 
NMT systems only rely on parallel corpora for parameter estimation . <S>
Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages , 
it is appealing to exploit monolingual corpora 
to improve NMT . <S>
We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data . <S>
The central idea is to reconstruct the monolingual corpora 
using an autoencoder , 
in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively . <S>
Our approach can not only exploit the monolingual corpora of the target language , 
but also of the source language . <S>
Experiments on the ChineseEnglish dataset show 
that our approach achieves significant improvements over state-of-the-art SMT and NMT systems . <S>
