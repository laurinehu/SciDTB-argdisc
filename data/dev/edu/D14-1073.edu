In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) 
using relevance prediction . <S>
We describe our 13 summarization methods each from one of four summarization strategies . <S>
We show 
how well our methods perform 
using Farsi text from the CLEF 2008 shared-task , 
which we translated to English automtatically . <S>
We report precision/recall/F1 , accuracy and time-on-task . <S>
We found 
that different summarization methods perform optimally for different evaluation metrics , 
but overall query biased word clouds are the best summarization strategy . <S>
In our analysis , we demonstrate 
that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions 
as our evaluation framework does . <S>
Finally , we present our recommendations 
for creating muchneeded evaluation standards and datasets . <S>
