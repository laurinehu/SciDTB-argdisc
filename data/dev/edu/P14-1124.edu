We aim to improve spoken term detection performance 
by incorporating contextual information beyond traditional N-gram language models . <S>
Instead of taking a broad view of topic context in spoken documents , 
variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents . <S>
We show 
that given the detection of one instance of a term 
we are more likely to find additional instances of that term in the same document . <S>
We leverage this burstiness of keywords 
by taking the most confident keyword hypothesis in each document 
and interpolating with lower scoring hits . <S>
We then develop a principled approach 
to select interpolation weights 
using only the ASR training data . <S>
Using this re-weighting approach 
we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program . <S>
