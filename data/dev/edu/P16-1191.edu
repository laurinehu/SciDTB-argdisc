Coarse-grained semantic categories 
such as supersenses 
have proven useful for a range of downstream tasks 
such as question answering or machine translation . <S>
To date , no effort has been put into integrating the supersenses into distributional word representations . <S>
We present a novel joint embedding model of words and supersenses , 
providing insights into the relationship between words and supersenses in the same vector space . <S>
Using these embeddings in a deep neural network model , 
we demonstrate 
that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks . <S>
