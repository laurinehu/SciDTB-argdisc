{
	"root": [
		{
			"id": 0,
			"parent": -1,
			"text": "ROOT",
			"relation": "null"
		},
		{
			"id": 1,
			"parent": 3,
			"text": "There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images . <S>",
			"relation": "contrast"
		},
		{
			"id": 2,
			"parent": 1,
			"text": "These tasks have focused on literal descriptions of the image . <S>",
			"relation": "elab-addition"
		},
		{
			"id": 3,
			"parent": 7,
			"text": "To move beyond the literal , ",
			"relation": "bg-goal"
		},
		{
			"id": 4,
			"parent": 5,
			"text": "we choose to explore ",
			"relation": "attribution"
		},
		{
			"id": 5,
			"parent": 3,
			"text": "how questions about an image are often directed at commonsense inference and the abstract events ",
			"relation": "enablement"
		},
		{
			"id": 6,
			"parent": 5,
			"text": "evoked by objects in the image . <S>",
			"relation": "elab-addition"
		},
		{
			"id": 7,
			"parent": 0,
			"text": "In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , ",
			"relation": "ROOT"
		},
		{
			"id": 8,
			"parent": 7,
			"text": "where the system is tasked ",
			"relation": "elab-addition"
		},
		{
			"id": 9,
			"parent": 8,
			"text": "with asking a natural and engaging question ",
			"relation": "manner-means"
		},
		{
			"id": 10,
			"parent": 9,
			"text": "when shown an image . <S>",
			"relation": "temporal"
		},
		{
			"id": 11,
			"parent": 7,
			"text": "We provide three datasets ",
			"relation": "elab-addition"
		},
		{
			"id": 12,
			"parent": 11,
			"text": "which cover a variety of images from object-centric to event-centric , with considerably more abstract training data ",
			"relation": "elab-addition"
		},
		{
			"id": 13,
			"parent": 12,
			"text": "than provided to state-of-the-art captioning systems thus far . <S>",
			"relation": "comparison"
		},
		{
			"id": 14,
			"parent": 7,
			"text": "We train and test several generative and retrieval models ",
			"relation": "elab-addition"
		},
		{
			"id": 15,
			"parent": 14,
			"text": "to tackle the task of VQG . <S>",
			"relation": "enablement"
		},
		{
			"id": 16,
			"parent": 18,
			"text": "Evaluation results show ",
			"relation": "attribution"
		},
		{
			"id": 17,
			"parent": 18,
			"text": "that while such models ask reasonable questions for a variety of images , ",
			"relation": "temporal"
		},
		{
			"id": 18,
			"parent": 7,
			"text": "there is still a wide gap with human performance ",
			"relation": "evaluation"
		},
		{
			"id": 19,
			"parent": 18,
			"text": "which motivates further work ",
			"relation": "elab-addition"
		},
		{
			"id": 20,
			"parent": 19,
			"text": "on connecting images with commonsense knowledge and pragmatics . <S>",
			"relation": "elab-addition"
		},
		{
			"id": 21,
			"parent": 7,
			"text": "Our proposed task offers a new challenge to the community ",
			"relation": "evaluation"
		},
		{
			"id": 22,
			"parent": 21,
			"text": "which we hope furthers interest ",
			"relation": "elab-addition"
		},
		{
			"id": 23,
			"parent": 22,
			"text": "in exploring deeper connections between vision & language . <S>",
			"relation": "elab-addition"
		}
	]
}