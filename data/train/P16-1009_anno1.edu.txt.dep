{
	"root": [
		{
			"id": 0,
			"parent": -1,
			"text": "ROOT",
			"relation": "null"
		},
		{
			"id": 1,
			"parent": 5,
			"text": "Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs , ",
			"relation": "bg-compare"
		},
		{
			"id": 2,
			"parent": 1,
			"text": "while only using parallel data for training . <S>",
			"relation": "condition"
		},
		{
			"id": 3,
			"parent": 5,
			"text": "Targetside monolingual data plays an important role ",
			"relation": "joint"
		},
		{
			"id": 4,
			"parent": 3,
			"text": "in boosting fluency for phrasebased statistical machine translation , ",
			"relation": "elab-addition"
		},
		{
			"id": 5,
			"parent": 0,
			"text": "and we investigate the use of monolingual data for NMT . <S>",
			"relation": "ROOT"
		},
		{
			"id": 6,
			"parent": 9,
			"text": "In contrast to previous work , ",
			"relation": "comparison"
		},
		{
			"id": 7,
			"parent": 6,
			"text": "which combines NMT models with separately trained language models , ",
			"relation": "elab-addition"
		},
		{
			"id": 8,
			"parent": 9,
			"text": "we note ",
			"relation": "attribution"
		},
		{
			"id": 9,
			"parent": 5,
			"text": "that encoder-decoder NMT architectures already have the capacity ",
			"relation": "elab-addition"
		},
		{
			"id": 10,
			"parent": 9,
			"text": "to learn the same information as a language model , ",
			"relation": "elab-addition"
		},
		{
			"id": 11,
			"parent": 9,
			"text": "and we explore strategies ",
			"relation": "joint"
		},
		{
			"id": 12,
			"parent": 11,
			"text": "to train with monolingual data ",
			"relation": "elab-addition"
		},
		{
			"id": 13,
			"parent": 12,
			"text": "without changing the neural network architecture . <S>",
			"relation": "condition"
		},
		{
			"id": 14,
			"parent": 15,
			"text": "By pairing monolingual training data with an automatic backtranslation , ",
			"relation": "manner-means"
		},
		{
			"id": 15,
			"parent": 5,
			"text": "we can treat it as additional parallel training data , ",
			"relation": "evaluation"
		},
		{
			"id": 16,
			"parent": 15,
			"text": "and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , ",
			"relation": "joint"
		},
		{
			"id": 17,
			"parent": 16,
			"text": "obtaining new state-of-the-art results . <S>",
			"relation": "elab-addition"
		},
		{
			"id": 18,
			"parent": 19,
			"text": "We also show ",
			"relation": "attribution"
		},
		{
			"id": 19,
			"parent": 5,
			"text": "that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German . <S>",
			"relation": "evaluation"
		}
	]
}
