{
	"root": [
		{
			"id": 0,
			"parent": -1,
			"text": "ROOT",
			"relation": "null"
		},
		{
			"id": 1,
			"parent": 0,
			"text": "We use single-agent and multi-agent Reinforcement Learning ( RL ) \r",
			"relation": "ROOT"
		},
		{
			"id": 2,
			"parent": 1,
			"text": "for learning dialogue policies in a resource allocation negotiation scenario . <S>\r",
			"relation": "enablement"
		},
		{
			"id": 3,
			"parent": 1,
			"text": "Two agents learn concurrently \r",
			"relation": "elab-aspect"
		},
		{
			"id": 4,
			"parent": 3,
			"text": "by interacting with each other \r",
			"relation": "manner-means"
		},
		{
			"id": 5,
			"parent": 3,
			"text": "without any need for simulated users ( SUs ) \r",
			"relation": "elab-addition"
		},
		{
			"id": 6,
			"parent": 5,
			"text": "to train against \r",
			"relation": "elab-addition"
		},
		{
			"id": 7,
			"parent": 5,
			"text": "or corpora \r",
			"relation": "same-unit"
		},
		{
			"id": 8,
			"parent": 7,
			"text": "to learn from . <S>\r",
			"relation": "elab-addition"
		},
		{
			"id": 9,
			"parent": 1,
			"text": "In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , \r",
			"relation": "evaluation"
		},
		{
			"id": 10,
			"parent": 9,
			"text": "varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate . <S>\r",
			"relation": "elab-addition"
		},
		{
			"id": 11,
			"parent": 13,
			"text": "Our results show \r",
			"relation": "attribution"
		},
		{
			"id": 12,
			"parent": 13,
			"text": "that generally Q-learning fails to converge \r",
			"relation": "contrast"
		},
		{
			"id": 13,
			"parent": 9,
			"text": "whereas PHC and PHC-WoLF always converge and perform similarly . <S>\r",
			"relation": "cause"
		},
		{
			"id": 14,
			"parent": 15,
			"text": "We also show \r",
			"relation": "attribution"
		},
		{
			"id": 15,
			"parent": 9,
			"text": "that very high gradually decreasing exploration rates are required for convergence . <S>\r",
			"relation": "cause"
		},
		{
			"id": 16,
			"parent": 17,
			"text": "We conclude \r",
			"relation": "attribution"
		},
		{
			"id": 17,
			"parent": 9,
			"text": "that multi-agent RL of dialogue policies is a promising alternative \r",
			"relation": "summary"
		},
		{
			"id": 18,
			"parent": 17,
			"text": "to using single-agent RL and SUs \r",
			"relation": "elab-addition"
		},
		{
			"id": 19,
			"parent": 18,
			"text": "or learning directly from corpora . <S>\r",
			"relation": "joint"
		}
	]
}