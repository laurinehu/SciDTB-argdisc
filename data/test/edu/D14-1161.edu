Many forms of word relatedness have been developed , 
providing different perspectives on word similarity . <S>
We introduce a Bayesian probabilistic tensor factorization model 
for synthesizing a single word vector representation 
and per-perspective linear transformations from any number of word similarity matrices . <S>
The resulting word vectors , 
when combined with the per-perspective linear transformation , 
approximately recreate 
while also regularizing and generalizing , 
each word similarity perspective . <S>
Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , 
and is capable of generalizing to words outside the vocabulary of any particular perspective . <S>
We evaluated the word embeddings with GRE antonym questions , 
the result achieves the state-of-the-art performance . <S>  
