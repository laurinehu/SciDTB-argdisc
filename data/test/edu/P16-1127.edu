We propose an approach for semantic parsing 
that uses a recurrent neural network 
to map a natural language question into a logical form representation of a KB query . <S>
Building on recent work by ( Wang et al. , 2015 ) , 
the interpretable logical forms , 
which are structured objects 
obeying certain constraints , 
are enumerated by an underlying grammar 
and are paired with their canonical realizations . <S>
In order to use sequence prediction , 
we need to sequentialize these logical forms . <S>
We compare three sequentializations : 
a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence 
consisting of derivation steps relative to the underlying grammar . <S>
We also show 
how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor . <S>
Our experiments show important improvements over previous results for the same dataset , 
and also demonstrate the advantage 
of incorporating the grammatical constraints . <S>
