Automatic metrics are widely used in machine translation as a substitute for human assessment . <S>
With the introduction of any new metric comes the question 
of just how well that metric mimics human assessment of translation quality . <S>
This is often measured by correlation with human judgment . <S>
Significance tests are generally not used 
to establish 
whether improvements over existing methods such as BLEU are statistically significant 
or have occurred simply by chance , however . <S>
In this paper , we introduce a significance test 
for comparing correlations of two metrics , along with an open-source implementation of the test . <S>
When applied to a range of metrics across seven language pairs , 
tests show 
that for a high proportion of metrics , there is insufficient evidence 
to conclude significant improvement over BLEU . <S>
