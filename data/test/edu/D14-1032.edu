Models 
that acquire semantic representations from both linguistic and perceptual input 
are of interest to researchers in NLP 
because of the obvious parallels with human language learning . <S>
Performance advantages of the multi-modal approach over language-only models have been clearly established 
when models are required to learn concrete noun concepts . <S>
However , such concepts are comparatively rare in everyday language . <S>
In this work , we present a new means 
of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts 
via an approach 
that learns multi-modal embeddings . <S>
Our architecture outperforms previous approaches in combining input from distinct modalities , 
and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives . <S>
We discuss the implications of our results 
both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation . <S>
