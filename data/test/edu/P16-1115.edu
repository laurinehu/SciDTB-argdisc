A common use of language is to refer to visually present objects . <S>
Modelling it in computers requires modelling the link between language and perception . <S>
The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , 
and composes the meaning of a phrase 
through composition of the denotations of its component words . <S>
It was recently shown to perform well in a game-playing scenario with a small number of object types . <S>
We apply it to two large sets of real-world photographs 
that contain a much larger variety of object types 
and for which referring expressions are available . <S>
Using a pre-trained convolutional neural network 
to extract image region features , 
and augmenting these with positional information , 
we show 
that the model achieves performance competitive with the state of the art in a reference resolution task 
( given expression , 
find bounding box of its referent ) , 
while , 
as we argue , 
being conceptually simpler and more flexible . <S>
