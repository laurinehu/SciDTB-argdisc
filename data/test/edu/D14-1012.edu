Recent work has shown success 
in using continuous word embeddings 
learned from unlabeled data 
as features 
to improve supervised NLP systems , 
which is regarded as a simple semi-supervised learning mechanism . <S>
However , fundamental problems 
on effectively incorporating the word embedding features within the framework of linear models 
remain . <S>
In this study , we investigate and analyze three different approaches , 
including a new proposed distributional prototype approach , 
for utilizing the embedding features . <S>
The presented approaches can be integrated into most of the classical linear models in NLP . <S>
Experiments on the task of named entity recognition show 
that each of the proposed approaches can better utilize the word embedding features , 
among which the distributional prototype approach performs the best . <S>
Moreover , the combination of the approaches provides additive improvements , 
outperforming the dense and continuous embedding features by nearly 2 points of F1 score . <S>
