We consider the task 
of learning a context dependent mapping from utterances to denotations . <S>
With only denotations at training time , we must search over a combinatorially large space of logical forms , 
which is even larger with context-dependent utterances . <S>
To cope with this challenge , 
we perform successive projections of the full model onto simpler models 
that operate over equivalence classes of logical forms . <S>
Though less expressive , 
we find 
that these simpler models are much faster 
and can be surprisingly effective . <S>
Moreover , they can be used 
to bootstrap the full model . <S>
Finally , we collected three new contextdependent semantic parsing datasets , 
and develop a new left-to-right parser . <S>
