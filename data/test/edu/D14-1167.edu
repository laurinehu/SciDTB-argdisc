We examine the embedding approach 
to reason new relational facts from a large-scale knowledge graph and a text corpus . <S>
We propose a novel method of jointly embedding entities and words into the same continuous vector space . <S>
The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus . <S>
Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space . <S>
Large scale experiments on Freebase and a Wikipedia/NY Times corpus show 
that jointly embedding brings promising improvement in the accuracy of predicting facts , 
compared to separately embedding knowledge graphs and text . <S>
Particularly , jointly embedding enables the prediction of facts 
containing entities out of the knowledge graph , 
which cannot be handled by previous embedding methods . <S>
At the same time , concerning the quality of the word embeddings , 
experiments on the analogical reasoning task show 
that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) . <S>  
